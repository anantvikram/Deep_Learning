{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using sequence-to-sequence with Keras\n",
    "\n",
    "Author: [Valentin Malykh](http://val.maly.hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Explained\n",
    "If we want to undertand something then paying attention is really helpful. In Neural Networks, it is also true if we can identify the most critical or important things to pay attention to. You can find the resources on [attention](http://ruder.io/deep-learning-nlp-best-practices/index.html#attention) and current best practices for NLP in general. Mathematically we can also visualize attention with the following image:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/philipperemy/keras-attention-mechanism/blob/master/assets/attention_1.png?raw=true\" width=\"400\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the big spike is where the attention of the model will be directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get activations of attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, layer_name=None):\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of simple random dataset for attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=None):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    if attention_column is None:\n",
    "        attention_column = np.random.randint(low=0, high=input_dim)\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = 2\n",
    "TIME_STEPS = 20\n",
    "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "APPLY_ATTENTION_BEFORE_LSTM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attention itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two models with attention, you could try both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_attention_applied_after_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    lstm_units = 32\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_attention_applied_before_lstm():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
    "    attention_mul = attention_3d_block(inputs)\n",
    "    lstm_units = 32\n",
    "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data and compile model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20, 2)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 2, 20)        0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 2, 20)        0           permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2, 20)        420         reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 2)        0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 20, 2)        0           input_2[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 32)           4480        attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            33          lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 4,933\n",
      "Trainable params: 4,933\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "N = 300000\n",
    "inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
    "\n",
    "if APPLY_ATTENTION_BEFORE_LSTM:\n",
    "    m = model_attention_applied_after_lstm()\n",
    "else:\n",
    "    m = model_attention_applied_before_lstm()\n",
    "\n",
    "m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model and get the attention visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/1\n",
      "270000/270000 [==============================] - 26s - loss: 0.4737 - acc: 0.7406 - val_loss: 0.5819 - val_acc: 0.6424\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAELCAYAAADeNe2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAaeK58jXgDsqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01d808d940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m.fit([inputs_1], outputs, epochs=1, batch_size=512, validation_split=0.1)\n",
    "\n",
    "attention_vectors = []\n",
    "for i in range(300):\n",
    "    testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
    "    attention_vector = np.mean(get_activations(m,\n",
    "                                               testing_inputs_1,\n",
    "                                               layer_name='attention_vec')[0], axis=2).squeeze()\n",
    "    # print('attention =', attention_vector)\n",
    "    assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
    "    attention_vectors.append(attention_vector)\n",
    "\n",
    "attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
    "# plot part.\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
    "                                                                     title='Attention Mechanism as '\n",
    "                                                                           'a function of input'\n",
    "                                                                           ' dimensions.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New task - translation\n",
    "\n",
    "The machine translation is old and well-known field in natural language processing. From the 1950s scientists tried to create a model to automatically translate from say French to English. Nowadays it became possible and the attention mechanism takes great part in that. Here the example image with attention map for the neural machine translation of sample phrase:\n",
    "<p align=\"center\">\n",
    "  <img src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/12/Screen-Shot-2015-12-30-at-1.23.48-PM.png\" width=\"400\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab we will concentrate on much simplier task: we will translate from human readable date to machine readable one.\n",
    "\n",
    "To do this we need to get one more concept - Sequence-to-Sequence language modeling.\n",
    "The idea of such architecture is here:\n",
    "<p aling=\"center\">\n",
    "<img src=\"https://talbaumel.github.io/attention/img/birnn.jpg\" width=\"400\">\n",
    "</p>\n",
    "\n",
    "There is an Embeding layer at the bottom, the bidirectional RNN in the middle and softmax as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENCODER_UNITS = 32\n",
    "DECODER_UNITS = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use more complex idea that simple seq2seq: we're adding two explicit parts of our network - encoder and decoder (which is applied attention on). The explanatory picture for this idea is below:\n",
    "<p aling=\"center\"><img src=\"https://i.stack.imgur.com/Zwsmz.png\"></p>\n",
    "\n",
    "The lower part of the network is encoding the input to some hidden intermediate representation and the upper part is decoing the hidвen representation into some readable output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets create a machine translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_simple_nmt(in_chars, out_chars):\n",
    "    inputs = Input(shape=(TIME_STEPS,))\n",
    "    \n",
    "    input_embed = Embedding(in_chars, ENCODER_UNITS * 2, input_length=TIME_STEPS, trainable=True,\n",
    "                            name='embedding')(inputs)\n",
    "    \n",
    "    enc_out = Bidirectional(LSTM(ENCODER_UNITS, return_sequences=True))(input_embed)\n",
    "    dec_out = LSTM(DECODER_UNITS, return_sequences=True)(enc_out)\n",
    "    attention_mul = attention_3d_block(dec_out)\n",
    "    \n",
    "    output = TimeDistributed(Dense(out_chars, activation='softmax'))(attention_mul)\n",
    "   \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generate data. It will be dates in different text formats and in fixed output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "fake.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS = ['short',\n",
    "           'medium',\n",
    "           'long',\n",
    "           'full',\n",
    "           'd MMM YYY', \n",
    "           'd MMMM YYY',\n",
    "           'dd MMM YYY',\n",
    "           'd MMM, YYY',\n",
    "           'd MMMM, YYY',\n",
    "           'dd, MMM YYY',\n",
    "           'd MM YY',\n",
    "           'd MMMM YYY',\n",
    "           'MMMM d YYY',\n",
    "           'MMMM d, YYY',\n",
    "           'dd.MM.YY']\n",
    "\n",
    "# change this if you want it to work with another language\n",
    "LOCALES = ['en_US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_date():\n",
    "    \"\"\"\n",
    "        Creates some fake dates \n",
    "        :returns: tuple containing human readable string, machine readable string, and date object\n",
    "    \"\"\"\n",
    "    dt = fake.date_object()\n",
    "\n",
    "    try:\n",
    "        human_readable = format_date(dt, format=random.choice(FORMATS), locale=random.choice(LOCALES))\n",
    "\n",
    "        case_change = random.choice([0,1,2])\n",
    "        if case_change == 1:\n",
    "            human_readable = human_readable.upper()\n",
    "        elif case_change == 2:\n",
    "            human_readable = human_readable.lower()\n",
    "        # if case_change == 0, do nothing\n",
    "\n",
    "        machine_readable = dt.isoformat()\n",
    "    except AttributeError as e:\n",
    "        return None, None, None\n",
    "\n",
    "    return human_readable, machine_readable, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(n_examples):\n",
    "    \"\"\"\n",
    "        Creates a dataset with n_examples and vocabularies\n",
    "        :n_examples: the number of examples to generate\n",
    "    \"\"\"\n",
    "    human_vocab = set()\n",
    "    machine_vocab = set()\n",
    "    dataset = []\n",
    "\n",
    "    for i in tqdm(range(n_examples)):\n",
    "        h, m, _ = create_date()\n",
    "        if h is not None:\n",
    "            dataset.append((h, m))\n",
    "            human_vocab.update(tuple(h))\n",
    "            machine_vocab.update(tuple(m))\n",
    "\n",
    "    human = dict(zip(list(human_vocab) + ['<unk>', '<pad>'], \n",
    "                     list(range(len(human_vocab) + 2))))\n",
    "    inv_machine = dict(enumerate(list(machine_vocab) + ['<unk>', '<pad>']))\n",
    "    machine = {v:k for k,v in inv_machine.items()}\n",
    " \n",
    "    return dataset, human, machine, inv_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_int(string, lenght, vocab):\n",
    "    if len(string) > lenght:\n",
    "        string = string[:lenght]\n",
    "        \n",
    "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
    "    \n",
    "    if len(string) < lenght:\n",
    "        rep += [vocab['<pad>']] * (lenght - len(string))\n",
    "    \n",
    "    return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def int_to_string(ints, inv_vocab):\n",
    "    return [inv_vocab[i] for i in ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually generating data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [00:28<00:00, 10396.21it/s]\n"
     ]
    }
   ],
   "source": [
    "N = 300000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and training model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 64)       3840        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 20, 64)       24832       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 20, 32)       12416       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, 32, 20)       0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 32, 20)       0           permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32, 20)       420         reshape_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 20, 32)       0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 20, 32)       0           lstm_4[0][0]                     \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 13)       429         attention_mul[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 41,937\n",
      "Trainable params: 41,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ti..., inputs=[<tf.Tenso...)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "m = model_simple_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, TIME_STEPS, human_vocab) for i in inputs])\n",
    "targets = [string_to_int(t, TIME_STEPS, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 270000 samples, validate on 30000 samples\n",
      "Epoch 1/1\n",
      "  6144/270000 [..............................] - ETA: 587s - loss: 2.2867 - acc: 0.4980"
     ]
    }
   ],
   "source": [
    "m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n",
    "\n",
    "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    prediction = np.argmax(prediction[0], axis=-1)\n",
    "    return int_to_string(prediction, inv_output_vocabulary)\n",
    "\n",
    "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
    "    predicted = []\n",
    "    for example in examples:\n",
    "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
    "        print('input:', example)\n",
    "        print('output:', predicted[-1])\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run_examples(m, human_vocab, inv_machine_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the actual attention map on some example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_map(model, input_vocabulary, inv_output_vocabulary, text):\n",
    "    \"\"\"\n",
    "        visualization of attention map\n",
    "    \"\"\"\n",
    "    # encode the string\n",
    "    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n",
    "\n",
    "    # get the output sequence\n",
    "    prediction = model.predict(np.array([encoded]))\n",
    "    predicted_text = np.argmax(prediction[0], axis=-1)\n",
    "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "\n",
    "    text_ = list(text)\n",
    "    # get the lengths of the string\n",
    "    input_length = len(text)\n",
    "    output_length = predicted_text.index('<pad>')\n",
    "    # get the activation map\n",
    "    attention_vector = get_activations(model, [encoded], layer_name='attention_vec')[0].squeeze()\n",
    "    activation_map = attention_vector[0:output_length, 0:input_length]\n",
    "    \n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(8, 8.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(activation_map, interpolation='nearest', cmap='gray')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Probability', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "    ax.set_yticks(range(output_length))\n",
    "    ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention_map(m, human_vocab, inv_machine_vocab, EXAMPLES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably see, the default model for this lab is not that good. But you could try to improve it by yourself. You could get better results, like this:\n",
    "\n",
    "<p align=\"center\"><img src=\"https://user-images.githubusercontent.com/6295292/26899949-bbac0c7c-4b9e-11e7-84d6-c2f31166af07.png\" width=\"800\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before doing this part run Kernel->Restart so the GPU memory is completely free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After the toy examples we finally see what is attention good for. We will try actual Neural Machine Translation model, which is shipped with this lab. This NMT model was trained on German-English corpus, so it will translate from German to English.\n",
    "\n",
    "But before we start we need to discuss one more think, which is really important in machine translation (and also other NLP tasks), this is BPE representation.\n",
    "\n",
    "### BPE\n",
    "BPE stands for byte pair enconding. It means that common byte pairs (bigrams of chars in our case) are replaced by the byte which never occur in the corpus. Say, in our corpus we have never seen \"#\" char, so we could use it to represent some typical bigram like \"ie\". But in practive all the printable chars are used, so for BPE the unprintable part of codepage is used. To actually print thу text, we need to reformat it back. so you'll see in text \"@@ \" - this is artefacts from such renormalisation.\n",
    "\n",
    "Here we have example text in German, which will be translated in English by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! head wmt/newstest2015.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual architecture of used is:\n",
    "![](../2017-09-14_23-11-48.png)\n",
    "It is slightly more complex than in our toy example with dates. Hуre we again have encoder-decoder architecture, but the attention now is taken from all the input, not the part of it. And also we use so called context vector which is representation of the whole sentence - it is helpful for the model to \"get the idea\" of a phrase before translating it.\n",
    "\n",
    "\n",
    "Lets finally see what our model will give us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!python OpenSeq2Seq/run.py --config_file=OpenSeq2Seq/example_configs/nmt.json --logdir=./nmt --mode=infer --inference_out=pred.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Acknowledgements__: code based on keras-visualize-activations of Philippe Remy\n",
    "\n",
    "URL: https://github.com/philipperemy/keras-visualize-activations\n",
    "\n",
    "The idea of date translation is borrowed from https://github.com/datalogue/keras-attention.\n",
    "\n",
    "For the real case we have used https://github.com/NVIDIA/OpenSeq2Seq, NVIDIA's implementation of Seq2Seq model.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
